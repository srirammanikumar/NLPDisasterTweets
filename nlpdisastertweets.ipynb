{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/sample_submission.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntrain_data.head(10)","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  \n5       1  \n6       1  \n7       1  \n8       1  \n9       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#flood #disaster Heavy rain causes flash flood...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm on top of the hill and I can see a fire in...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>14</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>There's an emergency evacuation happening now ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm afraid that the tornado is coming to our a...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Convert each tweet to words\n\nA method to remove all html tags, and tokenize the tweet. Remove stop words"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\n\nimport re\nfrom bs4 import BeautifulSoup\n\ndef tweet_to_words(review):\n    nltk.download(\"stopwords\", quiet=True)\n    stemmer = PorterStemmer()\n    \n    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n    words = text.split() # Split string into words\n    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n    words = [PorterStemmer().stem(w) for w in words] # stem\n    \n    return words","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing on one tweet\ntweet_to_words(train_data.text[10])","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"['three', 'peopl', 'die', 'heat', 'wave', 'far']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply it on all tweets\n\nwords_train = [tweet_to_words(tweet) for tweet in train_data.text]\nwords_test = [tweet_to_words(tweet) for tweet in test_data.text]","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a word dictionary\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking any one tweet\nwords_train[10]","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"['three', 'peopl', 'die', 'heat', 'wave', 'far']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get all the vocab from all the tweets\n# list the top 500 frequent words. with the count of occurance\n\ndef build_dict(data, vocab_size = 500):\n    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n    \n    # Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n    # sentence is a list of words.\n    '''\n    Creating a flat list - \n    for sublist in words_train:\n        for item in sublist:\n            flat_list.append(item)\n        \n    '''\n    flat_list = [item for sublist in data for item in sublist]\n    word_value, frequency = np.unique(flat_list, return_counts=True)\n    word_count = dict(zip(word_value, frequency)) # A dict storing the words that appear in the reviews along with how often they occur\n    \n    # TODO: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n    #       sorted_words[-1] is the least frequently appearing word.\n    \n    \n    sorted_list = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n    sorted_words = [item[0] for item in sorted_list]\n    \n    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n        word_dict[word] = idx + 2                              # 'infrequent' labels\n        \n    return word_dict","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_dict = build_dict(words_train)\nprint(word_dict)","execution_count":16,"outputs":[{"output_type":"stream","text":"{'co': 2, 'http': 3, 'like': 4, 'fire': 5, 'get': 6, 'bomb': 7, 'new': 8, 'via': 9, '2': 10, 'one': 11, 'go': 12, 'peopl': 13, 'news': 14, 'kill': 15, 'burn': 16, 'year': 17, 'video': 18, 'flood': 19, 'time': 20, 'crash': 21, 'emerg': 22, 'disast': 23, 'bodi': 24, 'attack': 25, 'build': 26, 'day': 27, 'fatal': 28, 'look': 29, 'say': 30, 'home': 31, 'love': 32, 'polic': 33, 'would': 34, '3': 35, 'u': 36, 'make': 37, 'famili': 38, 'evacu': 39, 'still': 40, 'storm': 41, 'train': 42, 'see': 43, 'us': 44, 'come': 45, 'back': 46, 'know': 47, 'california': 48, 'suicid': 49, '1': 50, 'bag': 51, 'live': 52, 'watch': 53, 'want': 54, 'collaps': 55, 'man': 56, 'world': 57, 'car': 58, 'death': 59, 'derail': 60, 'scream': 61, 'got': 62, 'rt': 63, 'first': 64, 'take': 65, 'caus': 66, 'let': 67, 'think': 68, 'nuclear': 69, 'two': 70, 'drown': 71, 'today': 72, 'war': 73, 'need': 74, 'work': 75, 'accid': 76, 'dead': 77, 'wreck': 78, 'deton': 79, 'youtub': 80, 'destroy': 81, '4': 82, '5': 83, 'hijack': 84, 'full': 85, 'plan': 86, 'feel': 87, 'hiroshima': 88, 'life': 89, 'old': 90, 'fuck': 91, 'good': 92, 'obliter': 93, 'fear': 94, 'help': 95, 'murder': 96, 'weapon': 97, 'may': 98, 'way': 99, 'surviv': 100, 'injuri': 101, 'last': 102, 'even': 103, 'wound': 104, '2015': 105, 'mani': 106, 'could': 107, 'devast': 108, 'rescu': 109, 'servic': 110, 'use': 111, 'die': 112, 'wildfir': 113, 'report': 114, 'explod': 115, 'hazard': 116, 'riot': 117, 'run': 118, 'w': 119, 'call': 120, 'read': 121, 'save': 122, 'mass': 123, 'wave': 124, 'best': 125, 'collid': 126, 'damag': 127, 'hostag': 128, 'right': 129, 'thing': 130, 'pleas': 131, 'quarantin': 132, 'anoth': 133, 'armi': 134, 'catastroph': 135, 'mh370': 136, 'crush': 137, 'hous': 138, 'lol': 139, 'realli': 140, 'school': 141, 'warn': 142, 'citi': 143, 'black': 144, 'fall': 145, 'miss': 146, 'water': 147, 'photo': 148, 'stop': 149, 'thank': 150, 'casualti': 151, 'forest': 152, 'hot': 153, '11': 154, '8': 155, 'delug': 156, 'god': 157, 'hit': 158, 'hope': 159, 'obama': 160, 'pm': 161, 'state': 162, 'confirm': 163, 'demolish': 164, 'electrocut': 165, 'much': 166, 'northern': 167, 'reddit': 168, 'tri': 169, 'cross': 170, 'never': 171, 'set': 172, '9': 173, 'bomber': 174, 'chang': 175, 'flame': 176, 'great': 177, 'legionnair': 178, 'play': 179, 'bioterror': 180, 'end': 181, 'head': 182, 'investig': 183, 'rain': 184, 'siren': 185, 'start': 186, 'desol': 187, 'japan': 188, 'latest': 189, 'releas': 190, '6': 191, 'area': 192, 'atom': 193, 'break': 194, 'everi': 195, 'injur': 196, 'issu': 197, 'show': 198, 'typhoon': 199, 'everyon': 200, 'near': 201, 'content': 202, 'im': 203, 'offici': 204, 'said': 205, 'terror': 206, 'thunderstorm': 207, 'top': 208, 'updat': 209, 'women': 210, 'annihil': 211, 'blaze': 212, 'ever': 213, 'face': 214, 'happen': 215, 'land': 216, 'sever': 217, 'shit': 218, 'wind': 219, 'girl': 220, 'oil': 221, 'post': 222, 'sign': 223, 'truck': 224, 'movi': 225, 'natur': 226, 'smoke': 227, 'boy': 228, 'charg': 229, 'check': 230, 'earthquak': 231, 'found': 232, 'game': 233, 'night': 234, 'offic': 235, 'sinc': 236, '10': 237, '15': 238, 'ass': 239, 'follow': 240, 'militari': 241, 'pick': 242, 'weather': 243, 'next': 244, 'respond': 245, 'stori': 246, 'well': 247, 'without': 248, 'debri': 249, 'declar': 250, 'explos': 251, 'keep': 252, 'littl': 253, 'malaysia': 254, 'suspect': 255, 'wild': 256, '7': 257, 'guy': 258, 'heat': 259, 'sink': 260, 'sound': 261, 'structur': 262, 'terrorist': 263, '08': 264, 'danger': 265, 'food': 266, 'lightn': 267, 'migrant': 268, 'refuge': 269, 'fan': 270, 'fight': 271, 'high': 272, 'hurrican': 273, 'job': 274, 'move': 275, 'road': 276, 'trap': 277, 'alway': 278, 'bad': 279, 'battl': 280, 'blood': 281, 'hail': 282, 'light': 283, 'made': 284, 'spill': 285, 'thunder': 286, 'week': 287, '70': 288, 'bloodi': 289, 'free': 290, 'market': 291, 'nation': 292, 'p': 293, 'red': 294, 'ruin': 295, 'also': 296, 'care': 297, 'hour': 298, 'inund': 299, 'kid': 300, 'leav': 301, 'loud': 302, 'someon': 303, 'air': 304, 'busi': 305, 'china': 306, 'failur': 307, 'gonna': 308, 'long': 309, 'lot': 310, 'minut': 311, 'power': 312, 'put': 313, 'shot': 314, 'summer': 315, 'tonight': 316, 'ambul': 317, 'babi': 318, 'big': 319, 'bridg': 320, 'collis': 321, 'destruct': 322, 'displac': 323, 'eye': 324, 'harm': 325, 'outbreak': 326, 'panic': 327, 'saudi': 328, 'sinkhol': 329, 'survivor': 330, 'tornado': 331, 'turn': 332, '30': 333, 'affect': 334, 'bu': 335, 'island': 336, 'order': 337, 'person': 338, 'phone': 339, 'whole': 340, 'wreckag': 341, '05': 342, '40': 343, 'around': 344, 'close': 345, 'drive': 346, 'drought': 347, 'heart': 348, 'islam': 349, 'landslid': 350, 'n': 351, 'ok': 352, 'part': 353, 'real': 354, 'rememb': 355, 'total': 356, 'windstorm': 357, 'hundr': 358, 'r': 359, 'rescuer': 360, 'send': 361, 'trauma': 362, 'twister': 363, 'wait': 364, 'word': 365, 'airplan': 366, 'away': 367, 'boat': 368, 'chemic': 369, 'counti': 370, 'dust': 371, 'effect': 372, 'friend': 373, 'group': 374, 'open': 375, 'self': 376, 'ship': 377, 'sunk': 378, 'white': 379, '0': 380, 'armageddon': 381, 'august': 382, 'bang': 383, 'bleed': 384, 'cliff': 385, 'curfew': 386, 'engulf': 387, 'famin': 388, 'half': 389, 'massacr': 390, 'mudslid': 391, 'pic': 392, 'plane': 393, 'possibl': 394, 'sandstorm': 395, 'stock': 396, 'thought': 397, 'twitter': 398, 'violent': 399, 'whirlwind': 400, 'woman': 401, 'apocalyps': 402, 'b': 403, 'better': 404, 'blast': 405, 'came': 406, 'deal': 407, 'intern': 408, 'iran': 409, 'least': 410, 'link': 411, 'memori': 412, 'mosqu': 413, 'raze': 414, 'rise': 415, 'saw': 416, 'search': 417, 'second': 418, 'tragedi': 419, 'traumatis': 420, 'troubl': 421, 'unit': 422, 'wanna': 423, 'anniversari': 424, 'blown': 425, 'fedex': 426, 'hear': 427, 'heard': 428, 'horribl': 429, 'oh': 430, 'past': 431, 'place': 432, 'secur': 433, 'song': 434, 'st': 435, 'stay': 436, 'sure': 437, 'tell': 438, 'tomorrow': 439, 'tsunami': 440, 'volcano': 441, 'zone': 442, 'ban': 443, 'beauti': 444, 'blew': 445, 'book': 446, 'cool': 447, 'flatten': 448, 'govern': 449, 'insur': 450, 'lava': 451, 'left': 452, 'meltdown': 453, 'must': 454, 'pandemonium': 455, 'panick': 456, 'peac': 457, 'river': 458, 'stand': 459, 'talk': 460, 'went': 461, '00': 462, 'actual': 463, 'calgari': 464, 'demolit': 465, 'ebay': 466, 'expect': 467, 'final': 468, 'give': 469, 'hellfir': 470, 'isi': 471, 'ladi': 472, 'men': 473, 'project': 474, 'reunion': 475, 'shoot': 476, 'shoulder': 477, 'someth': 478, 'soon': 479, 'support': 480, 'swallow': 481, 'believ': 482, 'c': 483, 'case': 484, 'cyclon': 485, 'dog': 486, 'e': 487, 'find': 488, 'human': 489, 'mean': 490, 'rainstorm': 491, 'south': 492, 'star': 493, 'street': 494, 'ur': 495, 'airport': 496, 'america': 497, 'american': 498, 'arson': 499}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform all tweets with each word to an integer corresponding to its rank in the words_dict\n# pad is 280 as the max word limit for each \n\ndef convert_and_pad(word_dict, sentence, pad=280):\n    NOWORD = 0 # We will use 0 to represent the 'no word' category\n    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n    \n    working_sentence = [NOWORD] * pad\n    \n    for word_index, word in enumerate(sentence[:pad]):\n        if word in word_dict:\n            working_sentence[word_index] = word_dict[word]\n        else:\n            working_sentence[word_index] = INFREQ\n            \n    return working_sentence, min(len(sentence), pad)\n\ndef convert_and_pad_data(word_dict, data, pad=280):\n    result = []\n    lengths = []\n    \n    for sentence in data:\n        converted, leng = convert_and_pad(word_dict, sentence, pad)\n        result.append(converted)\n        lengths.append(leng)\n        \n    return np.array(result), np.array(lengths)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, train_X_len = convert_and_pad_data(word_dict, words_train)\ntest_X, test_X_len = convert_and_pad_data(word_dict, words_test)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_X[200])\nprint(train_X_len[200])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}