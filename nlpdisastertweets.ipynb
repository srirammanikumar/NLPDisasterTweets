{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/train.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntrain_data.head(10)","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  \n5       1  \n6       1  \n7       1  \n8       1  \n9       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#flood #disaster Heavy rain causes flash flood...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm on top of the hill and I can see a fire in...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>14</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>There's an emergency evacuation happening now ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm afraid that the tornado is coming to our a...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove all NaN \ntrain_data = train_data.fillna('a')\ntest_data = test_data.fillna('a')","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert each tweet to words\n\nA method to remove all html tags, and tokenize the tweet. Remove stop words"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\n\nimport re\nfrom bs4 import BeautifulSoup\n\ndef tweet_to_words(review):\n    nltk.download(\"stopwords\", quiet=True)\n    stemmer = PorterStemmer()\n    \n    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n    words = text.split() # Split string into words\n    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n    words = [PorterStemmer().stem(w) for w in words] # stem\n    \n    return words","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing on one tweet\ntweet_to_words(train_data.text[10])","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"['three', 'peopl', 'die', 'heat', 'wave', 'far']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply it on all tweets and keywords\n\nwords_train = [tweet_to_words(tweet) for tweet in train_data.text]\nwords_test = [tweet_to_words(tweet) for tweet in test_data.text]\n\nkeywords_train = [tweet_to_words(tweet) for tweet in train_data.keyword]\nkeywords_test = [tweet_to_words(tweet) for tweet in test_data.keyword]","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a word dictionary\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking any one tweet\nwords_train[10]","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"['three', 'peopl', 'die', 'heat', 'wave', 'far']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get all the vocab from all the tweets\n# list the top 500 frequent words. with the count of occurance\n\ndef build_dict(data, vocab_size = 500):\n    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n    \n    # Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n    # sentence is a list of words.\n    '''\n    Creating a flat list - \n    for sublist in words_train:\n        for item in sublist:\n            flat_list.append(item)\n        \n    '''\n    flat_list = [item for sublist in data for item in sublist]\n    word_value, frequency = np.unique(flat_list, return_counts=True)\n    word_count = dict(zip(word_value, frequency)) # A dict storing the words that appear in the reviews along with how often they occur\n    \n    # TODO: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n    #       sorted_words[-1] is the least frequently appearing word.\n    \n    \n    sorted_list = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n    sorted_words = [item[0] for item in sorted_list]\n    \n    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n        word_dict[word] = idx + 2                              # 'infrequent' labels\n        \n    return word_dict","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_dict = build_dict(words_train)\nprint(word_dict)","execution_count":10,"outputs":[{"output_type":"stream","text":"{'co': 2, 'http': 3, 'like': 4, 'fire': 5, 'get': 6, 'bomb': 7, 'new': 8, 'via': 9, '2': 10, 'one': 11, 'go': 12, 'peopl': 13, 'news': 14, 'kill': 15, 'burn': 16, 'year': 17, 'video': 18, 'flood': 19, 'time': 20, 'crash': 21, 'emerg': 22, 'disast': 23, 'bodi': 24, 'attack': 25, 'build': 26, 'day': 27, 'fatal': 28, 'look': 29, 'say': 30, 'home': 31, 'love': 32, 'polic': 33, 'would': 34, '3': 35, 'u': 36, 'make': 37, 'famili': 38, 'evacu': 39, 'still': 40, 'storm': 41, 'train': 42, 'see': 43, 'us': 44, 'come': 45, 'back': 46, 'know': 47, 'california': 48, 'suicid': 49, '1': 50, 'bag': 51, 'live': 52, 'watch': 53, 'want': 54, 'collaps': 55, 'man': 56, 'world': 57, 'car': 58, 'death': 59, 'derail': 60, 'scream': 61, 'got': 62, 'rt': 63, 'first': 64, 'take': 65, 'caus': 66, 'let': 67, 'think': 68, 'nuclear': 69, 'two': 70, 'drown': 71, 'today': 72, 'war': 73, 'need': 74, 'work': 75, 'accid': 76, 'dead': 77, 'wreck': 78, 'deton': 79, 'youtub': 80, 'destroy': 81, '4': 82, '5': 83, 'hijack': 84, 'full': 85, 'plan': 86, 'feel': 87, 'hiroshima': 88, 'life': 89, 'old': 90, 'fuck': 91, 'good': 92, 'obliter': 93, 'fear': 94, 'help': 95, 'murder': 96, 'weapon': 97, 'may': 98, 'way': 99, 'surviv': 100, 'injuri': 101, 'last': 102, 'even': 103, 'wound': 104, '2015': 105, 'mani': 106, 'could': 107, 'devast': 108, 'rescu': 109, 'servic': 110, 'use': 111, 'die': 112, 'wildfir': 113, 'report': 114, 'explod': 115, 'hazard': 116, 'riot': 117, 'run': 118, 'w': 119, 'call': 120, 'read': 121, 'save': 122, 'mass': 123, 'wave': 124, 'best': 125, 'collid': 126, 'damag': 127, 'hostag': 128, 'right': 129, 'thing': 130, 'pleas': 131, 'quarantin': 132, 'anoth': 133, 'armi': 134, 'catastroph': 135, 'mh370': 136, 'crush': 137, 'hous': 138, 'lol': 139, 'realli': 140, 'school': 141, 'warn': 142, 'citi': 143, 'black': 144, 'fall': 145, 'miss': 146, 'water': 147, 'photo': 148, 'stop': 149, 'thank': 150, 'casualti': 151, 'forest': 152, 'hot': 153, '11': 154, '8': 155, 'delug': 156, 'god': 157, 'hit': 158, 'hope': 159, 'obama': 160, 'pm': 161, 'state': 162, 'confirm': 163, 'demolish': 164, 'electrocut': 165, 'much': 166, 'northern': 167, 'reddit': 168, 'tri': 169, 'cross': 170, 'never': 171, 'set': 172, '9': 173, 'bomber': 174, 'chang': 175, 'flame': 176, 'great': 177, 'legionnair': 178, 'play': 179, 'bioterror': 180, 'end': 181, 'head': 182, 'investig': 183, 'rain': 184, 'siren': 185, 'start': 186, 'desol': 187, 'japan': 188, 'latest': 189, 'releas': 190, '6': 191, 'area': 192, 'atom': 193, 'break': 194, 'everi': 195, 'injur': 196, 'issu': 197, 'show': 198, 'typhoon': 199, 'everyon': 200, 'near': 201, 'content': 202, 'im': 203, 'offici': 204, 'said': 205, 'terror': 206, 'thunderstorm': 207, 'top': 208, 'updat': 209, 'women': 210, 'annihil': 211, 'blaze': 212, 'ever': 213, 'face': 214, 'happen': 215, 'land': 216, 'sever': 217, 'shit': 218, 'wind': 219, 'girl': 220, 'oil': 221, 'post': 222, 'sign': 223, 'truck': 224, 'movi': 225, 'natur': 226, 'smoke': 227, 'boy': 228, 'charg': 229, 'check': 230, 'earthquak': 231, 'found': 232, 'game': 233, 'night': 234, 'offic': 235, 'sinc': 236, '10': 237, '15': 238, 'ass': 239, 'follow': 240, 'militari': 241, 'pick': 242, 'weather': 243, 'next': 244, 'respond': 245, 'stori': 246, 'well': 247, 'without': 248, 'debri': 249, 'declar': 250, 'explos': 251, 'keep': 252, 'littl': 253, 'malaysia': 254, 'suspect': 255, 'wild': 256, '7': 257, 'guy': 258, 'heat': 259, 'sink': 260, 'sound': 261, 'structur': 262, 'terrorist': 263, '08': 264, 'danger': 265, 'food': 266, 'lightn': 267, 'migrant': 268, 'refuge': 269, 'fan': 270, 'fight': 271, 'high': 272, 'hurrican': 273, 'job': 274, 'move': 275, 'road': 276, 'trap': 277, 'alway': 278, 'bad': 279, 'battl': 280, 'blood': 281, 'hail': 282, 'light': 283, 'made': 284, 'spill': 285, 'thunder': 286, 'week': 287, '70': 288, 'bloodi': 289, 'free': 290, 'market': 291, 'nation': 292, 'p': 293, 'red': 294, 'ruin': 295, 'also': 296, 'care': 297, 'hour': 298, 'inund': 299, 'kid': 300, 'leav': 301, 'loud': 302, 'someon': 303, 'air': 304, 'busi': 305, 'china': 306, 'failur': 307, 'gonna': 308, 'long': 309, 'lot': 310, 'minut': 311, 'power': 312, 'put': 313, 'shot': 314, 'summer': 315, 'tonight': 316, 'ambul': 317, 'babi': 318, 'big': 319, 'bridg': 320, 'collis': 321, 'destruct': 322, 'displac': 323, 'eye': 324, 'harm': 325, 'outbreak': 326, 'panic': 327, 'saudi': 328, 'sinkhol': 329, 'survivor': 330, 'tornado': 331, 'turn': 332, '30': 333, 'affect': 334, 'bu': 335, 'island': 336, 'order': 337, 'person': 338, 'phone': 339, 'whole': 340, 'wreckag': 341, '05': 342, '40': 343, 'around': 344, 'close': 345, 'drive': 346, 'drought': 347, 'heart': 348, 'islam': 349, 'landslid': 350, 'n': 351, 'ok': 352, 'part': 353, 'real': 354, 'rememb': 355, 'total': 356, 'windstorm': 357, 'hundr': 358, 'r': 359, 'rescuer': 360, 'send': 361, 'trauma': 362, 'twister': 363, 'wait': 364, 'word': 365, 'airplan': 366, 'away': 367, 'boat': 368, 'chemic': 369, 'counti': 370, 'dust': 371, 'effect': 372, 'friend': 373, 'group': 374, 'open': 375, 'self': 376, 'ship': 377, 'sunk': 378, 'white': 379, '0': 380, 'armageddon': 381, 'august': 382, 'bang': 383, 'bleed': 384, 'cliff': 385, 'curfew': 386, 'engulf': 387, 'famin': 388, 'half': 389, 'massacr': 390, 'mudslid': 391, 'pic': 392, 'plane': 393, 'possibl': 394, 'sandstorm': 395, 'stock': 396, 'thought': 397, 'twitter': 398, 'violent': 399, 'whirlwind': 400, 'woman': 401, 'apocalyps': 402, 'b': 403, 'better': 404, 'blast': 405, 'came': 406, 'deal': 407, 'intern': 408, 'iran': 409, 'least': 410, 'link': 411, 'memori': 412, 'mosqu': 413, 'raze': 414, 'rise': 415, 'saw': 416, 'search': 417, 'second': 418, 'tragedi': 419, 'traumatis': 420, 'troubl': 421, 'unit': 422, 'wanna': 423, 'anniversari': 424, 'blown': 425, 'fedex': 426, 'hear': 427, 'heard': 428, 'horribl': 429, 'oh': 430, 'past': 431, 'place': 432, 'secur': 433, 'song': 434, 'st': 435, 'stay': 436, 'sure': 437, 'tell': 438, 'tomorrow': 439, 'tsunami': 440, 'volcano': 441, 'zone': 442, 'ban': 443, 'beauti': 444, 'blew': 445, 'book': 446, 'cool': 447, 'flatten': 448, 'govern': 449, 'insur': 450, 'lava': 451, 'left': 452, 'meltdown': 453, 'must': 454, 'pandemonium': 455, 'panick': 456, 'peac': 457, 'river': 458, 'stand': 459, 'talk': 460, 'went': 461, '00': 462, 'actual': 463, 'calgari': 464, 'demolit': 465, 'ebay': 466, 'expect': 467, 'final': 468, 'give': 469, 'hellfir': 470, 'isi': 471, 'ladi': 472, 'men': 473, 'project': 474, 'reunion': 475, 'shoot': 476, 'shoulder': 477, 'someth': 478, 'soon': 479, 'support': 480, 'swallow': 481, 'believ': 482, 'c': 483, 'case': 484, 'cyclon': 485, 'dog': 486, 'e': 487, 'find': 488, 'human': 489, 'mean': 490, 'rainstorm': 491, 'south': 492, 'star': 493, 'street': 494, 'ur': 495, 'airport': 496, 'america': 497, 'american': 498, 'arson': 499}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform all tweets with each word to an integer corresponding to its rank in the words_dict\n# pad is 280 as the max word limit for each \n\ndef convert_and_pad(word_dict, sentence, pad=280):\n    NOWORD = 0 # We will use 0 to represent the 'no word' category\n    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n    \n    working_sentence = [NOWORD] * pad\n    \n    for word_index, word in enumerate(sentence[:pad]):\n        if word in word_dict:\n            working_sentence[word_index] = word_dict[word]\n        else:\n            working_sentence[word_index] = INFREQ\n            \n    return working_sentence, min(len(sentence), pad)\n\ndef convert_and_pad_data(word_dict, data, pad=280):\n    result = []\n    lengths = []\n    \n    for sentence in data:\n        converted, leng = convert_and_pad(word_dict, sentence, pad)\n        result.append(converted)\n        lengths.append(leng)\n        \n    return np.array(result), np.array(lengths)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, train_X_len = convert_and_pad_data(word_dict, words_train, 100)\ntest_X, test_X_len = convert_and_pad_data(word_dict, words_test, 100)\nkeyword_train_rank, dummy = convert_and_pad_data(word_dict,keywords_train,1)\nkeyword_test_rank, dummy = convert_and_pad_data(word_dict,keywords_test,1)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_X[1])\nprint(train_X_len[1])","execution_count":13,"outputs":[{"output_type":"stream","text":"[152   5 201   1   1   1   1   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0]\n7\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_data['word_ranks'] = train_X\n# train_data['tweet_size'] = train_X_len\n\ntrain_df = pd.concat([pd.DataFrame(train_X_len), pd.DataFrame(keyword_train_rank), pd.DataFrame(train_X)], axis=1)\ntrain_df","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"      0   0    0    1    2    3    4    5    6    7   ...  90  91  92  93  94  \\\n0      7   0    1    1  231   98    1    1   44    0  ...   0   0   0   0   0   \n1      7   0  152    5  201    1    1    1    1    0  ...   0   0   0   0   0   \n2     11   0    1    1    1  432    1  235   39    1  ...   0   0   0   0   0   \n3      8   0    1    1   13    1  113   39  337   48  ...   0   0   0   0   0   \n4      9   0   62    1  148    1    1  227  113    1  ...   0   0   0   0   0   \n...   ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...  ..  ..  ..  ..  ..   \n7608  11   0   70    1    1    1  320   55    1   31  ...   0   0   0   0   0   \n7609  12   0    1    1    1    1  256    5   48  103  ...   0   0   0   0   0   \n7610  11   0    1    1    1    1    1    1  441    1  ...   0   0   0   0   0   \n7611  17   0   33  183  487    1  126   58  253    1  ...   0   0   0   0   0   \n7612  11   0  189   31  414  167   48  113    1   14  ...   0   0   0   0   0   \n\n      95  96  97  98  99  \n0      0   0   0   0   0  \n1      0   0   0   0   0  \n2      0   0   0   0   0  \n3      0   0   0   0   0  \n4      0   0   0   0   0  \n...   ..  ..  ..  ..  ..  \n7608   0   0   0   0   0  \n7609   0   0   0   0   0  \n7610   0   0   0   0   0  \n7611   0   0   0   0   0  \n7612   0   0   0   0   0  \n\n[7613 rows x 102 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n      <th>95</th>\n      <th>96</th>\n      <th>97</th>\n      <th>98</th>\n      <th>99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>231</td>\n      <td>98</td>\n      <td>1</td>\n      <td>1</td>\n      <td>44</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>0</td>\n      <td>152</td>\n      <td>5</td>\n      <td>201</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>432</td>\n      <td>1</td>\n      <td>235</td>\n      <td>39</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>13</td>\n      <td>1</td>\n      <td>113</td>\n      <td>39</td>\n      <td>337</td>\n      <td>48</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>0</td>\n      <td>62</td>\n      <td>1</td>\n      <td>148</td>\n      <td>1</td>\n      <td>1</td>\n      <td>227</td>\n      <td>113</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7608</th>\n      <td>11</td>\n      <td>0</td>\n      <td>70</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>320</td>\n      <td>55</td>\n      <td>1</td>\n      <td>31</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7609</th>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>256</td>\n      <td>5</td>\n      <td>48</td>\n      <td>103</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7610</th>\n      <td>11</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>441</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7611</th>\n      <td>17</td>\n      <td>0</td>\n      <td>33</td>\n      <td>183</td>\n      <td>487</td>\n      <td>1</td>\n      <td>126</td>\n      <td>58</td>\n      <td>253</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7612</th>\n      <td>11</td>\n      <td>0</td>\n      <td>189</td>\n      <td>31</td>\n      <td>414</td>\n      <td>167</td>\n      <td>48</td>\n      <td>113</td>\n      <td>1</td>\n      <td>14</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7613 rows × 102 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_label = train_data.target\ntrain_label.head()","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"0    1\n1    1\n2    1\n3    1\n4    1\nName: target, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.utils.data\n\n# Turn the input pandas dataframe into tensors\ntrain_y = torch.from_numpy(train_label.values).float().squeeze()\ntrain_X = torch.from_numpy(train_df.values).long()\n\n# Build the dataset\ntrain_ds = torch.utils.data.TensorDataset(train_X, train_y)\n# Build the dataloader\ntrain_dl = torch.utils.data.DataLoader(train_ds)v","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\nclass LSTMClassifier(nn.Module):\n    \"\"\"\n    This is the simple RNN model to check for real disaster\n    \"\"\"\n\n    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n        \"\"\"\n        Initialize the model by settingg up the various layers.\n        \"\"\"\n        super(LSTMClassifier, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n        self.dense = nn.Linear(in_features=hidden_dim, out_features=1)\n        self.sig = nn.Sigmoid()\n        \n        self.word_dict = None\n\n    def forward(self, x):\n        \"\"\"\n        Perform a forward pass of our model on some input.\n        \"\"\"\n        x = x.t()\n        lengths = x[0,:]\n        reviews = x[1:,:]\n        embeds = self.embedding(reviews)\n        lstm_out, _ = self.lstm(embeds)\n        out = self.dense(lstm_out)\n        out = out[lengths - 1, range(len(lengths))]\n        return self.sig(out.squeeze())","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, train_loader, epochs, optimizer, loss_fn, device):\n    for epoch in range(1, epochs + 1):\n        model.train()\n        total_loss = 0\n        for batch in train_loader:         \n            batch_X, batch_y = batch\n            \n            batch_X = batch_X.to(device)\n            batch_y = batch_y.to(device)\n            \n            optimizer.zero_grad()\n            \n            output = model.forward(batch_X)\n            \n            loss = loss_fn(output, batch_y)\n            loss.backward()\n            \n            optimizer.step()\n            \n            total_loss += loss.data.item()\n        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\n# from train.model import LSTMClassifier\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = LSTMClassifier(102, 100, 500).to(device)\noptimizer = optim.Adam(model.parameters())\nloss_fn = torch.nn.BCELoss()\n\ntrain(model, train_dl, 5, optimizer, loss_fn, device)","execution_count":19,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])) is deprecated. Please ensure they have the same size.\n  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n","name":"stderr"},{"output_type":"stream","text":"Epoch: 1, BCELoss: 0.5794119907909573\nEpoch: 2, BCELoss: 0.4698074878912207\nEpoch: 3, BCELoss: 0.39927378615749887\nEpoch: 4, BCELoss: 0.3328204752278715\nEpoch: 5, BCELoss: 0.2641004881464704\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(keyword_test_rank), pd.DataFrame(test_X)], axis=1)\n\n\n# Turn the input pandas dataframe into tensors\n\ntest_X = torch.from_numpy(test_df.values).long()\n\n# Build the dataset\ntest_ds = torch.utils.data.TensorDataset(test_X)\n# Build the dataloader\ntest_dl = torch.utils.data.DataLoader(train_ds)\n\nfor batch in test_dl:\n    prediction = model(batch)\n    print(prediction)\n# train_ds","execution_count":25,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'DataLoader' object has no attribute 't'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-642a2e5e3b33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# train_ds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-e001349c6982>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mPerform\u001b[0m \u001b[0ma\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m \u001b[0mof\u001b[0m \u001b[0mour\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mon\u001b[0m \u001b[0msome\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \"\"\"\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 't'"]}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}